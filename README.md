
# üå§Ô∏è dbt-weather-poc

Pipeline analytique M√©t√©o-France ‚Äî ingestion, historisation et mod√©lisation de donn√©es horaires ‚Äî bas√© sur **Python**, **DuckDB**, **dbt** et **streamlit**.

Ce projet a un objectif simple : **d√©montrer, de bout en bout, la ma√Ætrise d‚Äôun workflow moderne dbt**, depuis la collecte des donn√©es jusqu‚Äô√† leur exposition en BI.

---

## Ce que ce projet met en ≈ìuvre c√¥t√© dbt

Ce repository illustre concr√®tement :

* **Sources d√©clar√©es** avec contr√¥le de fra√Æcheur (`loaded_at_field`)
* **Tests dbt** : not_null, unique, relationships, contraintes m√©tier, tests g√©n√©riques
* **Contrats de sch√©ma** sur les mod√®les critiques
* **Organisation modulaire** : `staging ‚Üí intermediate ‚Üí marts`
* **Mod√®les incr√©mentaux** (strat√©gie `merge`)
* **Macros personnalis√©es** (features m√©t√©o, conversions, casts, time series analysis)
* **Seeds** (√©chelle de Beaufort)
* **Exposures** (dashboard Streamlit comme consommateur final)
* **Documentation dbt** (descriptions, docs blocks, lineage graph)
* **Facteurs m√©tier** : dimensions stations & vent, table de faits horaire

L‚Äôobjectif n‚Äôest pas la BI en tant que produit, mais **la d√©monstration des bonnes pratiques dbt dans un pipeline r√©aliste**.

---

## Architecture globale

```
M√©t√©o-France API
    ‚Üì
Ingestion Python
    ‚Üì
DuckDB (raw.*)
    ‚Üì
dbt (staging ‚Üí intermediate ‚Üí marts)
    ‚Üì
Dashboard Streamlit (exposure)
```

---

## üöÄ Mise en route

### 1) Installer l‚Äôenvironnement

```bash
make env-setup
```

### 2) Activer l‚Äôenvironnement

```bash
source .venv/bin/activate
```

### 3) Variables d‚Äôenvironnement

Cr√©er `.env` :

```bash
METEOFRANCE_TOKEN=xxxxxxxxxxxx
DUCKDB_PATH=data/warehouse.duckdb
```
avec :
- `METEOFRANCE_TOKEN` : la cl√© API M√©t√©o-France  
- `DUCKDB_PATH` : le chemin du fichier DuckDB (par d√©faut `data/warehouse.duckdb`)
- 
### 4) Activer le profil dbt

```bash
export DBT_PROFILES_DIR=./profiles
```

---

## Ingestion des donn√©es (API ‚Üí DuckDB)

### Lancer une ingestion d√©partementale
```bash
make dwh-ingest DEPT=75
```
**Ce que fait la commande :**

* cr√©e `warehouse.duckdb` si absent ;
* interroge l‚ÄôAPI M√©t√©o-France (derni√®res **24 h**) pour le d√©partement `DEPT` ;
* √©crit en **brut** dans `raw.stations` et `raw.obs_hourly`.

**Garantie de ‚Äúraw‚Äù :**

* ‚úÖ Noms de colonnes **strictement identiques** √† la source (aucun renommage, aucun `lower/strip`)
* ‚úÖ Types **inchang√©s** (strings le cas √©ch√©ant)
* ‚úÖ Aucune normalisation d‚Äôunit√©s / s√©mantique (fait plus tard en **staging dbt**)
* ‚ûï Champs ajout√©s par l‚Äôingestion : `load_time` (UTC) et `dept_code`

**Idempotence & d√©duplication :**

* Les doublons sont emp√™ch√©s via la cl√© logique
  `(validity_time, geo_id_insee, reference_time)` : seules les lignes nouvelles sont ajout√©es.

---

### Mesurer la fra√Æcheur des sources dans le datawarehouse DuckDB

```bash
make dbt-sources-freshness
```

**Comment √ßa marche :**

* dbt lit `loaded_at_field: load_time` (d√©fini dans `sources.yml`)
* compare `load_time` √† l‚Äôhorloge actuelle, et applique les seuils :

  * ‚ö†Ô∏è **warn** si `load_time` > **2 h** 
    * donn√©es en retard (surveillance conseill√©e)
  * ‚õî **error** si `load_time` > **4 h**
    * pipeline consid√©r√© **en √©chec**
  *  ‚úÖ **pass** sinon
     *  la source est √† jour

**Que faire en cas d‚Äôalerte/erreur ?**

1. Relancer l‚Äôingestion :

   ```bash
   make dwh-ingest DEPT=9
   ```
2. V√©rifier le token API et la connectivit√© r√©seau.

---

### Ex√©cuter les tests sources

```bash
make dbt-sources-test
```
Les tests effectu√©es sont ceux d√©clar√©s dans `sources.yml`.

---

## üîé Inspection du DataWarehouse (DuckDB)

### Lister l'ensemble des tables
```bash
make dwh-tables
```
Cela permet de visualiser les schemas et noms de toutes les tables du DataWarehouse.

### Afficher les colonnes d'une table
```bash
make dwh-table-info TABLE=raw.stations
```
Cela permet de visualiser les noms des colonnes de la table `TABLE` et leur type.

### Afficher un extrait d'une table
```bash
make dwh-table-sample TABLE=raw.stations
```
Permet de visualiser un extrait des donn√©es de la table `TABLE` directement dans le terminal.

### Afficher les dimensions d'une table
```bash
make dwh-table-shape TABLE=raw.stations
```
Permet de visualiser le nombre de lignes et de colonnes de la table `TABLE`.

### Afficher toute les infos d'une table
```bash
make dwh-table TABLE=raw.stations
```
Permet de visualiser colonnes + dimensions + extrait de la table `TABLE`.

### Explorer le warehouse avec DuckDB CLI

#### Installation du client DuckDB
```bash
brew install duckdb
```

#### Ouvrir le shell interactif
```bash
duckdb warehouse.duckdb
```

#### Commandes utiles
```sql
show;                                  -- liste les tables
select count(*) from raw.stations;     -- compte les lignes d'une table
select * from raw.obs_hourly limit 5;  -- aper√ßu des donn√©es
show raw.stations;                     -- affiche le sch√©ma d'une table
```

---

## üß© Mod√©lisation dbt

### Structure

* `staging` : nettoyage, typage, renommage clair
* `intermediate` : calculs interm√©diaires, features m√©t√©o
* `marts` : tables analytiques et dimensionnelles

### Mod√®les cl√©s

* **`fct_obs_hourly`** (table de faits horaire)
* **`dim_stations`** (dimension g√©ographique des stations)

### Mod√®les incr√©mentaux

Deux mod√®les utilisent `materialized: incremental` avec strat√©gie `merge`
pour √©viter un full refresh syst√©matique.

Forcer un rebuild complet :

```bash
make dbt-rebuild
```

---

## ‚öôÔ∏è dbt ‚Äî ex√©cution par actions

### 1) Tester la connexion au DWH
```bash
dbt debug
```

### 2) Lancer l‚Äôex√©cution de tous les mod√®les
```bash
make dbt-build     # deps + run
```

### 2.bis) Ex√©cuter un sous-ensemble de mod√®les

```bash
dbt run --select stg_obs_hourly      # un mod√®le
dbt run --select tag:stg     # tous les mod√®les ayant le tag `stg`
dbt run --full-refresh -s tag:int # full refresh cibl√©
```

### 3) Lancer les tests

```bash
make dbt-test    # tous les tests
dbt test -s tag:mart  # tous les mod√®les ayant le tag `mart`
```

### 4) Lancer un rebuild complet

```bash
make dbt-rebuild    # reset + deps + run --full-refresh + test
```

---

## üìö Documentation dbt

Une fois les mod√®les ex√©cut√©s (`make dbt-rebuild`), on peut g√©n√©rer et explorer la
documentation dbt (mod√®les, sources, tests, lineage).

### G√©n√©rer la documentation

```bash
make dbt-docs-generate
```

Cela cr√©e les fichiers HTML/JSON de documentation dans le dossier `target/`.

### Servir la documentation en local

```bash
make dbt-docs-serve
```

Puis ouvrir le navigateur sur :

* [http://localhost:8080](http://localhost:8080)

On y retrouve :

* la liste des sources et mod√®les (staging, intermediate, marts) ;
* les descriptions de tables et de colonnes d√©finies dans les fichiers YAML ;
* les tests associ√©s ;
* le **graph de lineage** permettant de visualiser le flux `raw ‚Üí staging ‚Üí intermediate ‚Üí marts`. Accessible via le bouton ¬´ Lineage ¬ª en bas √† droite du panneau dbt Docs : <img src="docs/images/lineage-graph-icon.png" width="50">


---

## üìä Dashboard Streamlit (exposure dbt)

Une fois les donn√©es ing√©r√©es et les mod√®les dbt ex√©cut√©s, on peut explorer les marts via une petite app Streamlit.

### Lancer le dashboard

```bash
streamlit run apps/bi-streamlit/app.py
```

URL par d√©faut :
[http://localhost:8501](http://localhost:8501)

Ce dashboard s'appuie sur :
* `fct_obs_hourly`

### Exposure associ√©

Le dashboard est d√©clar√© comme **exposure dbt** (`weather_bi_streamlit`), permettant de :

* cibler uniquement les mod√®les qui l‚Äôalimentent :

  ```bash
  dbt ls -s +exposure:weather_bi_streamlit
  ```
* ex√©cuter uniquement ce p√©rim√®tre :

  ```bash
  dbt run -s +exposure:weather_bi_streamlit
  dbt test -s +exposure:weather_bi_streamlit
  ```

---

## üß∞ Makefile

Toutes les commandes du projet sont disponibles via **Makefile** :

```bash
make help
```

---

## üß± Scope & limites

Ce projet :

* ne vise pas √† produire une BI m√©tier aboutie,
* n‚Äôembarque pas (encore) d‚Äôorchestration ni CI/CD cloud,
* sert d‚Äôexemple p√©dagogique pour d√©montrer la ma√Ætrise dbt.

---

## üî≠ Prochaines √©volutions

* Snapshots (SCD2 sur `dim_stations`)
* CI/CD (tests + docs + artefacts)
* Migration cloud (BigQuery / Snowflake / Postgres manag√©)
* Am√©lioration du dashboard (UX & insights m√©tier)

---

## üë§ Auteur

Coralie Martinez

---

## Annexes

### üß∞ Scripts ingestion

#### `scripts/ingestion/fetch_meteofrance_paquetobs.py`

Client fetch-only :
- Appels API `/liste-stations` et `/paquet/horaire`
- Parsing CSV **sans aucune transformation**
- Retourne des DataFrames RAW

#### `scripts/ingestion/write_duckdb_raw.py`

Writer vers DuckDB :
- cr√©ation du sch√©ma `raw`
- `load_time` et `dept_code` ajout√©s
- d√©duplication sur PK logique

---

**Auteur :** Coralie Martinez
